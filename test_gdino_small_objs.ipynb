{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "Model loaded from C:\\Users\\divya\\.cache\\huggingface\\hub\\models--ShilongLiu--GroundingDINO\\snapshots\\a94c9b567a2a374598f05c584e96798a170c56fb\\groundingdino_swint_ogc.pth \n",
      " => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n"
     ]
    }
   ],
   "source": [
    "from absl import app, logging\n",
    "from PIL import Image as PILImg\n",
    "from PIL import ImageFilter\n",
    "import PIL.ImageOps\n",
    "from robokit.utils import annotate, overlay_masks\n",
    "from robokit.ObjDetection import GroundingDINOObjectPredictor,SegmentAnythingPredictor\n",
    "import torch \n",
    "gdino = GroundingDINOObjectPredictor(use_vitb=False, threshold=0.15)\n",
    "SAM = SegmentAnythingPredictor(vit_model=\"vit_h\")\n",
    "# Path to the input image\n",
    "image_path = 'D:/CODE/NIDS-Net/test_data/test_1/test_039.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Open the image and convert to RGB format\n",
      "INFO:absl:GDINO: Predict bounding boxes, phrases, and confidence scores\n",
      "INFO:absl:GDINO post processing\n",
      "INFO:absl:Annotate the scaled image with bounding boxes, confidence scores, and labels, and display\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 1, 6144, 8192])\n"
     ]
    }
   ],
   "source": [
    "text_prompt = 'objects'\n",
    "\n",
    "try:\n",
    "    logging.info(\"Open the image and convert to RGB format\")\n",
    "    image_pil = PILImg.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    logging.info(\"GDINO: Predict bounding boxes, phrases, and confidence scores\")\n",
    "    bboxes, phrases, gdino_conf = gdino.predict(image_pil, text_prompt)\n",
    "\n",
    "    logging.info(\"GDINO post processing\")\n",
    "    w, h = image_pil.size # Get image width and height \n",
    "    # Scale bounding boxes to match the original image size\n",
    "    image_pil_bboxes = gdino.bbox_to_scaled_xyxy(bboxes, w, h)  \n",
    "    \n",
    "    # logging.info(\"SAM: Predict\")\n",
    "    image_pil_bboxes, masks = SAM.predict(image_pil, image_pil_bboxes)\n",
    "    print(masks.shape)\n",
    "    \n",
    "    # print(image_pil_bboxes)\n",
    "    logging.info(\"Annotate the scaled image with bounding boxes, confidence scores, and labels, and display\")\n",
    "    bbox_annotated_pil = annotate(image_pil, image_pil_bboxes, gdino_conf, phrases)\n",
    "    bbox_annotated_pil.show()\n",
    "    del image_pil_bboxes\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle unexpected errors\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\divya/.cache\\torch\\hub\\mhamilton723_FeatUp_main\n",
      "Using cache found in C:\\Users\\divya/.cache\\torch\\hub\\facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from featup.util import norm, unnorm\n",
    "from featup.util import pca, remove_axes\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_feats(image, lr, hr):\n",
    "    assert len(image.shape) == len(lr.shape) == len(hr.shape) == 3\n",
    "    [lr_feats_pca, hr_feats_pca], _ = pca([lr.unsqueeze(0), hr.unsqueeze(0)])\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(150, 50))\n",
    "    ax[0].imshow(image.permute(1, 2, 0).detach().cpu())\n",
    "    ax[0].set_title(\"Image\")\n",
    "    ax[1].imshow(lr_feats_pca[0].permute(1, 2, 0).detach().cpu())\n",
    "    ax[1].set_title(\"Original Features\")\n",
    "    ax[2].imshow(hr_feats_pca[0].permute(1, 2, 0).detach().cpu())\n",
    "    ax[2].set_title(\"Upsampled Features\")\n",
    "    plt.show()\n",
    "\n",
    "image_path = 'D:/CODE/NIDS-Net/test_data/test_1/test_039.jpg'\n",
    "\n",
    "input_size = 448\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_norm = True\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(input_size),\n",
    "    T.CenterCrop((input_size, input_size)),\n",
    "    T.ToTensor(),\n",
    "    norm\n",
    "])\n",
    "\n",
    "image_tensor = transform(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "\n",
    "upsampler = torch.hub.load(\"mhamilton723/FeatUp\", 'dino16', use_norm=use_norm).to(device)\n",
    "hr_feats = upsampler(image_tensor)\n",
    "lr_feats = upsampler.model(image_tensor)\n",
    "plot_feats(unnorm(image_tensor)[0], lr_feats[0], hr_feats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
