{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Optional\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import transforms as pth_transforms\n",
    "from tqdm import tqdm, trange\n",
    "import time\n",
    "import torchvision.transforms as T\n",
    "\n",
    "sys.path.append(\"../detectron2\")\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "sys.path.append(\"../dinov2-main\")\n",
    "sys.path.append(\".\")\n",
    "from utils.visualizer import ColorMode, Visualizer\n",
    "from utils.instance_det_dataset import RealWorldDataset\n",
    "from utils.inference_utils import compute_similarity, stableMatching, \\\n",
    "    get_object_proposal, getColor, create_instances, nms, apply_nms, get_features\n",
    "from adapter import ModifiedClipAdapter, WeightAdapter\n",
    "logger = logging.getLogger(\"dinov2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser(\n",
    "        description: Optional[str] = None,\n",
    "        parents: Optional[List[argparse.ArgumentParser]] = [],\n",
    "        add_help: bool = True,\n",
    "):\n",
    "\n",
    "    parents = []\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=description,\n",
    "        parents=parents,\n",
    "        add_help=add_help,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_path\",\n",
    "        default=\"../database_mini/train\",\n",
    "        type=str,\n",
    "        help=\"Path to train dataset.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test_path\",\n",
    "        default=\"../database_mini/test\",\n",
    "        type=str,\n",
    "        help=\"Path to test dataset.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--imsize\",\n",
    "        default=224,\n",
    "        type=int,\n",
    "        help=\"Image size\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        default=\"./output\",\n",
    "        type=str,\n",
    "        help=\"Path to save outputs.\")\n",
    "    parser.add_argument(\"--num_workers\", default=0, type=int, help=\"Number of data loading workers per GPU.\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--gather-on-cpu\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to gather the train features on cpu, slower\"\n",
    "             \"but useful to avoid OOM for large datasets (e.g. ImageNet22k).\",\n",
    "    )\n",
    "\n",
    "    parser.set_defaults(\n",
    "        train_dataset=\"Object\",\n",
    "        test_dataset=\"Scene\",\n",
    "        batch_size=1,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[8]:\n",
    "# set the scene level here\n",
    "scene_level = 'all'  # all / easy / hard\n",
    "# Default args and initialize model\n",
    "args_parser = get_args_parser(description=\"Grounded SAM-DINOv2 Instance Detection\")\n",
    "imsize = 448\n",
    "tag = \"mask\"  # bbox\n",
    "args = args_parser.parse_args(args=[\n",
    "                                    \"--train_path\", \"C:/dataset/InsDet-FULL/Objects\",\n",
    "                                    \"--test_path\", \"C:/dataset/InsDet-FULL/Data/test_1_\"+scene_level,  # test_002\n",
    "                                    \"--output_dir\", \"exps/eval_ffa_\"+scene_level+\"4_gdino0.15t_vitl14_reg_\" + str(imsize) + \"_\" + tag,\n",
    "                                    ])\n",
    "os.makedirs(args.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14_reg')\n",
    "# encoder.to('cuda')\n",
    "# encoder.eval()\n",
    "\n",
    "use_adapter = False\n",
    "adapter_type = \"weight\"\n",
    "if use_adapter:\n",
    "    input_features = 1024 #768, 1024, the vector dimension\n",
    "    if adapter_type == \"clip\":\n",
    "        # adapter_args = 'Ins_clip_ratio_0.6_temp_0.05_epoch_40_lr_0.0001_bs_1024_vec_reduction_4_L2e4_vitl_reg'\n",
    "        adapter_args = 'Ins_0413__ratio_0.6_temp_0.05_epoch_40_lr_0.0001_bs_512_vec_reduction_4_L2e4_vitl_reg'\n",
    "        model_path = 'adapter_weights/adapter2FC/' + adapter_args + '_weights.pth'\n",
    "        adapter = ModifiedClipAdapter(input_features, reduction=4, ratio=0.6).to('cuda')\n",
    "    elif adapter_type == \"weight\":\n",
    "        adapter_args = 'Ins_weighted_10sigmoid_ratio_0.6_temp_0.05_epoch_40_lr_0.001_bs_1024_vec_reduction_4_L2e4_vitl_reg'\n",
    "        model_path = 'adapter_weights/adapter2FC/' + adapter_args + '_weights.pth'\n",
    "        adapter = WeightAdapter(input_features, reduction=4).to('cuda')\n",
    "\n",
    "\n",
    "    # Load the weights\n",
    "    adapter.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # If you plan to only evaluate the model, switch to eval mode\n",
    "    adapter.eval()\n",
    "\n",
    "    print('Model weights loaded and model is set to evaluation mode.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded D:/CODE/NIDS-Net/feats\\object_features_featup_patch.json\n",
      "object_features:  torch.Size([2400, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\divya\\AppData\\Local\\Temp\\ipykernel_24444\\2327795215.py:11: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  object_features = torch.Tensor(feat_dict['features']).cuda()\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'D:/CODE/NIDS-Net/feats'\n",
    "json_filename = 'object_features_featup_patch.json'\n",
    "if use_adapter:\n",
    "    output_dir = './adapted_obj_feats'\n",
    "    json_filename = adapter_args+'.json'\n",
    "\n",
    "with open(os.path.join(output_dir, json_filename), 'r') as f:\n",
    "    feat_dict = json.load(f)\n",
    "\n",
    "print(f\"Loaded {os.path.join(output_dir, json_filename)}\")\n",
    "object_features = torch.Tensor(feat_dict['features']).cuda()\n",
    "object_features = nn.functional.normalize(object_features, dim=1, p=2)\n",
    "print(\"object_features: \", object_features.shape) # Shape (2400, 384)\n",
    "do_matching = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[9]:\n",
    "\n",
    "# transform = pth_transforms.Compose([pth_transforms.ToTensor(),])\n",
    "# object_dataset = RealWorldDataset(args.train_path, args.train_dataset, transform=transform, imsize=args.imsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "Model loaded from C:\\Users\\divya\\.cache\\huggingface\\hub\\models--ShilongLiu--GroundingDINO\\snapshots\\a94c9b567a2a374598f05c584e96798a170c56fb\\groundingdino_swint_ogc.pth \n",
      " => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n"
     ]
    }
   ],
   "source": [
    "from absl import app, logging\n",
    "from PIL import Image as PILImg\n",
    "from robokit.ObjDetection import GroundingDINOObjectPredictor, SegmentAnythingPredictor\n",
    "\n",
    "logging.info(\"Initialize object detectors\")\n",
    "gdino = GroundingDINOObjectPredictor(use_vitb=False, threshold=0.15)\n",
    "\n",
    "from utils.inference_utils import get_foreground_mask\n",
    "\n",
    "image_dir = []\n",
    "proposals_list = []\n",
    "scene_name_list = []\n",
    "# source_list = sorted(glob.glob(os.path.join(args.test_path, '*')))\n",
    "transform = pth_transforms.Compose([pth_transforms.ToTensor(),])\n",
    "scene_features_list = []\n",
    "source_dir = os.path.join(args.test_path, 'images')\n",
    "\n",
    "image_paths = sorted([p for p in glob.glob(os.path.join(source_dir, '*'))\n",
    "                      if re.search('/*\\.(jpg|jpeg|png|gif|bmp|pbm)', str(p))])\n",
    "image_dir.extend(image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [02:00<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "all_bboxes = []\n",
    "text_prompt='objects'\n",
    "\n",
    "for image_path in tqdm(image_paths):\n",
    "    \n",
    "    image_pil = PILImg.open(image_path).convert(\"RGB\")\n",
    "    scene_name = os.path.basename(image_path).split('.')[0]\n",
    "    scene_name_list.append(scene_name)\n",
    "\n",
    "    logging.info(\"GDINO: Predict bounding boxes, phrases, and confidence scores\")\n",
    "    with torch.no_grad():\n",
    "        bboxes, phrases, gdino_conf = gdino.predict(image_pil, text_prompt)\n",
    "        # logging.info(\"GDINO post processing\")\n",
    "        w, h = image_pil.size  # Get image width and height\n",
    "        # Scale bounding boxes to match the original image size\n",
    "        image_pil_bboxes = gdino.bbox_to_scaled_xyxy(bboxes, w, h)\n",
    "        \n",
    "        all_bboxes.append(image_pil_bboxes.detach().cpu())\n",
    "        \n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbs = [x.tolist() for x in all_bboxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = { \n",
    "    'bboxes': bbs\n",
    "}\n",
    "with open(os.path.join(\"D:/CODE/NIDS-Net/feats/gdino_scaled_bboxes_all.json\"), \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"D:/CODE/NIDS-Net/feats/gdino_scaled_bboxes_all.json\"), 'r') as f:\n",
    "    gdino_bboxes = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdino_bboxes:  160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3264.148193359375, 2443.725830078125, 3493.134033203125, 3093.709228515625],\n",
       " [4688.62451171875, 1761.1258544921875, 4898.86669921875, 2109.93505859375],\n",
       " [2698.519775390625, 2580.19140625, 3125.491455078125, 2951.232421875],\n",
       " [6158.375, 2385.10009765625, 6723.3505859375, 2688.09814453125],\n",
       " [4349.1904296875, 1774.622314453125, 4553.83203125, 2124.205078125],\n",
       " [1955.602294921875, 2555.4833984375, 2495.739013671875, 2808.126953125],\n",
       " [3840.252197265625, 1877.4078369140625, 5017.3271484375, 2721.423095703125],\n",
       " [5376.2724609375, 1866.12939453125, 5482.8466796875, 2200.681640625],\n",
       " [4881.65771484375, 1830.8978271484375, 5290.85986328125, 2086.666748046875],\n",
       " [5906.4013671875, 2067.20361328125, 6094.470703125, 2259.49267578125],\n",
       " [2359.98828125, 2462.744873046875, 2663.0791015625, 2667.027099609375],\n",
       " [2729.56298828125, 2353.977783203125, 2945.7470703125, 2565.425537109375],\n",
       " [6247.95068359375, 2864.652587890625, 6884.43115234375, 3535.409423828125],\n",
       " [5078.0029296875, 1612.201171875, 7988.9228515625, 5589.349609375],\n",
       " [5451.36376953125, 2191.59521484375, 5586.26611328125, 2300.3701171875],\n",
       " [2486.662353515625, 1339.601806640625, 3505.998779296875, 2470.873291015625],\n",
       " [4907.60986328125, 2240.404541015625, 5155.94482421875, 2311.554443359375],\n",
       " [6416.8388671875, 1006.383544921875, 8090.95703125, 2754.989990234375],\n",
       " [6747.24755859375, 1006.779052734375, 8087.17333984375, 2752.721923828125],\n",
       " [4845.13818359375, 2381.48681640625, 5116.92529296875, 2497.99365234375],\n",
       " [6459.5908203125, 3272.784912109375, 6944.0634765625, 3693.228271484375],\n",
       " [323.380615234375, -2.8245849609375, 2511.146484375, 3271.05859375],\n",
       " [5055.5986328125, 2022.11962890625, 5829.1611328125, 2214.20068359375],\n",
       " [5468.64892578125, 2105.61865234375, 5824.86376953125, 2210.05419921875],\n",
       " [5049.6064453125, 2051.273681640625, 5129.8994140625, 2231.223876953125],\n",
       " [2386.402587890625, 2529.3857421875, 2664.042236328125, 2665.583984375],\n",
       " [4882.23095703125, 1826.482177734375, 5290.39697265625, 2256.803466796875],\n",
       " [6457.00439453125, 3270.091796875, 6945.10400390625, 3948.39453125],\n",
       " [2481.55810546875, 1336.066162109375, 4086.32421875, 4078.308837890625],\n",
       " [1859.72802734375, 1826.0113525390625, 6344.03271484375, 5636.20556640625],\n",
       " [1863.818603515625, 1031.67138671875, 8067.912109375, 5629.10498046875]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"gdino_bboxes: \", len(gdino_bboxes['bboxes']))\n",
    "gdino_bboxes['bboxes'][62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "del gdino\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAM = SegmentAnythingPredictor(vit_model=\"vit_h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. bboxes.shape=(16, 4)\n",
      "0. masks.shape=torch.Size([16, 6144, 8192]) || accurate_bboxs.shape=torch.Size([16, 4])\n",
      "==========================================\n",
      "1. bboxes.shape=(15, 4)\n",
      "1. masks.shape=torch.Size([15, 6144, 8192]) || accurate_bboxs.shape=torch.Size([15, 4])\n",
      "==========================================\n",
      "2. bboxes.shape=(21, 4)\n",
      "2. masks.shape=torch.Size([21, 6144, 8192]) || accurate_bboxs.shape=torch.Size([21, 4])\n",
      "==========================================\n",
      "3. bboxes.shape=(22, 4)\n",
      "3. masks.shape=torch.Size([22, 6144, 8192]) || accurate_bboxs.shape=torch.Size([22, 4])\n",
      "==========================================\n",
      "4. bboxes.shape=(22, 4)\n",
      "4. masks.shape=torch.Size([22, 6144, 8192]) || accurate_bboxs.shape=torch.Size([22, 4])\n",
      "==========================================\n",
      "5. bboxes.shape=(21, 4)\n",
      "5. masks.shape=torch.Size([21, 6144, 8192]) || accurate_bboxs.shape=torch.Size([21, 4])\n",
      "==========================================\n",
      "6. bboxes.shape=(24, 4)\n",
      "6. masks.shape=torch.Size([24, 6144, 8192]) || accurate_bboxs.shape=torch.Size([24, 4])\n",
      "==========================================\n",
      "7. bboxes.shape=(22, 4)\n",
      "7. masks.shape=torch.Size([22, 6144, 8192]) || accurate_bboxs.shape=torch.Size([22, 4])\n",
      "==========================================\n",
      "8. bboxes.shape=(22, 4)\n",
      "8. masks.shape=torch.Size([22, 6144, 8192]) || accurate_bboxs.shape=torch.Size([22, 4])\n",
      "==========================================\n",
      "9. bboxes.shape=(32, 4)\n",
      "9. masks.shape=torch.Size([32, 6144, 8192]) || accurate_bboxs.shape=torch.Size([32, 4])\n",
      "==========================================\n",
      "10. bboxes.shape=(19, 4)\n",
      "10. masks.shape=torch.Size([19, 6144, 8192]) || accurate_bboxs.shape=torch.Size([19, 4])\n",
      "==========================================\n",
      "11. bboxes.shape=(19, 4)\n",
      "11. masks.shape=torch.Size([19, 6144, 8192]) || accurate_bboxs.shape=torch.Size([19, 4])\n",
      "==========================================\n",
      "12. bboxes.shape=(23, 4)\n",
      "12. masks.shape=torch.Size([23, 6144, 8192]) || accurate_bboxs.shape=torch.Size([23, 4])\n",
      "==========================================\n",
      "13. bboxes.shape=(25, 4)\n",
      "13. masks.shape=torch.Size([25, 6144, 8192]) || accurate_bboxs.shape=torch.Size([25, 4])\n",
      "==========================================\n",
      "14. bboxes.shape=(25, 4)\n",
      "14. masks.shape=torch.Size([25, 6144, 8192]) || accurate_bboxs.shape=torch.Size([25, 4])\n",
      "==========================================\n",
      "15. bboxes.shape=(24, 4)\n",
      "15. masks.shape=torch.Size([24, 6144, 8192]) || accurate_bboxs.shape=torch.Size([24, 4])\n",
      "==========================================\n",
      "16. bboxes.shape=(22, 4)\n",
      "16. masks.shape=torch.Size([22, 6144, 8192]) || accurate_bboxs.shape=torch.Size([22, 4])\n",
      "==========================================\n",
      "17. bboxes.shape=(15, 4)\n",
      "17. masks.shape=torch.Size([15, 6144, 8192]) || accurate_bboxs.shape=torch.Size([15, 4])\n",
      "==========================================\n",
      "18. bboxes.shape=(17, 4)\n",
      "18. masks.shape=torch.Size([17, 6144, 8192]) || accurate_bboxs.shape=torch.Size([17, 4])\n",
      "==========================================\n",
      "19. bboxes.shape=(15, 4)\n",
      "19. masks.shape=torch.Size([15, 6144, 8192]) || accurate_bboxs.shape=torch.Size([15, 4])\n",
      "==========================================\n",
      "20. bboxes.shape=(24, 4)\n",
      "20. masks.shape=torch.Size([24, 6144, 8192]) || accurate_bboxs.shape=torch.Size([24, 4])\n",
      "==========================================\n",
      "21. bboxes.shape=(25, 4)\n",
      "21. masks.shape=torch.Size([25, 6144, 8192]) || accurate_bboxs.shape=torch.Size([25, 4])\n",
      "==========================================\n",
      "22. bboxes.shape=(24, 4)\n",
      "22. masks.shape=torch.Size([24, 6144, 8192]) || accurate_bboxs.shape=torch.Size([24, 4])\n",
      "==========================================\n",
      "23. bboxes.shape=(31, 4)\n",
      "23. masks.shape=torch.Size([31, 6144, 8192]) || accurate_bboxs.shape=torch.Size([31, 4])\n",
      "==========================================\n",
      "24. bboxes.shape=(36, 4)\n",
      "24. masks.shape=torch.Size([36, 6144, 8192]) || accurate_bboxs.shape=torch.Size([36, 4])\n",
      "==========================================\n",
      "25. bboxes.shape=(26, 4)\n",
      "25. masks.shape=torch.Size([26, 6144, 8192]) || accurate_bboxs.shape=torch.Size([26, 4])\n",
      "==========================================\n",
      "26. bboxes.shape=(43, 4)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 8.06 GiB. GPU 0 has a total capacity of 6.00 GiB of which 1.34 GiB is free. Of the allocated memory 2.68 GiB is allocated by PyTorch, and 939.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbboxes\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 16\u001b[0m   image_pil_bboxes, masks \u001b[38;5;241m=\u001b[39m \u001b[43mSAM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_pil\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m     18\u001b[0m accurate_bboxs \u001b[38;5;241m=\u001b[39m masks_to_bboxes(masks)\n",
      "File \u001b[1;32md:\\CODE\\NIDS-Net\\robokit\\ObjDetection.py:239\u001b[0m, in \u001b[0;36mSegmentAnythingPredictor.predict\u001b[1;34m(self, image, prompt_bboxes)\u001b[0m\n\u001b[0;32m    237\u001b[0m     transformed_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mapply_boxes_torch(input_boxes, image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_image(image)\n\u001b[1;32m--> 239\u001b[0m     masks, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_torch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpoint_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpoint_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformed_boxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmultimask_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    246\u001b[0m     input_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\divya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\divya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mobile_sam\\predictor.py:239\u001b[0m, in \u001b[0;36mSamPredictor.predict_torch\u001b[1;34m(self, point_coords, point_labels, boxes, mask_input, multimask_output, return_logits)\u001b[0m\n\u001b[0;32m    230\u001b[0m low_res_masks, iou_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmask_decoder(\n\u001b[0;32m    231\u001b[0m     image_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures,\n\u001b[0;32m    232\u001b[0m     image_pe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mprompt_encoder\u001b[38;5;241m.\u001b[39mget_dense_pe(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    235\u001b[0m     multimask_output\u001b[38;5;241m=\u001b[39mmultimask_output,\n\u001b[0;32m    236\u001b[0m )\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# Upscale the masks to the original image resolution\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlow_res_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moriginal_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_logits:\n\u001b[0;32m    242\u001b[0m     masks \u001b[38;5;241m=\u001b[39m masks \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmask_threshold\n",
      "File \u001b[1;32mc:\\Users\\divya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mobile_sam\\modeling\\sam.py:162\u001b[0m, in \u001b[0;36mSam.postprocess_masks\u001b[1;34m(self, masks, input_size, original_size)\u001b[0m\n\u001b[0;32m    155\u001b[0m masks \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(\n\u001b[0;32m    156\u001b[0m     masks,\n\u001b[0;32m    157\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_encoder\u001b[38;5;241m.\u001b[39mimg_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_encoder\u001b[38;5;241m.\u001b[39mimg_size),\n\u001b[0;32m    158\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    159\u001b[0m     align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    160\u001b[0m )\n\u001b[0;32m    161\u001b[0m masks \u001b[38;5;241m=\u001b[39m masks[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : input_size[\u001b[38;5;241m0\u001b[39m], : input_size[\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m--> 162\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbilinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m masks\n",
      "File \u001b[1;32mc:\\Users\\divya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:4087\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   4081\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mare_deterministic_algorithms_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_cuda:\n\u001b[0;32m   4082\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put\u001b[39;00m\n\u001b[0;32m   4083\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[0;32m   4084\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[0;32m   4085\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39m_upsample_linear_vec(\n\u001b[0;32m   4086\u001b[0m                 \u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[1;32m-> 4087\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample_bilinear2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 8.06 GiB. GPU 0 has a total capacity of 6.00 GiB of which 1.34 GiB is free. Of the allocated memory 2.68 GiB is allocated by PyTorch, and 939.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from PIL import Image as PILImg\n",
    "from utils.img_utils import masks_to_bboxes\n",
    "from robokit.utils import annotate, overlay_masks\n",
    "\n",
    "all_crops = []\n",
    "\n",
    "for i, image_path in enumerate(image_paths):\n",
    "\n",
    "    crops_dict = dict() \n",
    "\n",
    "    image_pil = PILImg.open(image_path).convert(\"RGB\")\n",
    "    bboxes = np.array(gdino_bboxes['bboxes'][i])\n",
    "    print(f\"{i}. {bboxes.shape=}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      image_pil_bboxes, masks = SAM.predict(image_pil, bboxes)\n",
    "    masks = masks.squeeze(1).detach().cpu()\n",
    "    accurate_bboxs = masks_to_bboxes(masks)\n",
    "    accurate_bboxs = torch.tensor(accurate_bboxs).detach().cpu()\n",
    "    print(f\"{i}. {masks.shape=} || {accurate_bboxs.shape=}\")\n",
    "    print(\"==========================================\")\n",
    "    rois, sel_rois, cropped_imgs, cropped_masks = get_object_proposal(image_path, accurate_bboxs, masks, tag=tag, ratio=0.25, save_rois=False, output_dir=args.output_dir)\n",
    "    crops_dict[\"rois\"] = rois\n",
    "    crops_dict[\"sel_rois\"] = sel_rois\n",
    "    crops_dict[\"cropped_imgs\"] = cropped_imgs\n",
    "    crops_dict[\"cropped_masks\"] = cropped_masks\n",
    "    \n",
    "    all_crops.append(crops_dict)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"rois_cropped_results.json\"), \"w\") as f:\n",
    "    json.dump(all_crops, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from featup.util import norm, unnorm, pca, remove_axes\n",
    "def FFA_preprocess(x_list, img_size=336):\n",
    "\n",
    "    preprocessed_images = []\n",
    "    for x in x_list:\n",
    "        # width, height = x.size\n",
    "        new_width = img_size\n",
    "        new_height = img_size\n",
    "\n",
    "        def _to_rgb(x):\n",
    "            if x.mode != \"RGB\":\n",
    "                x = x.convert(\"RGB\")\n",
    "            return x\n",
    "\n",
    "        # preprocessed_image = torchvision.transforms.Compose([\n",
    "        #     _to_rgb,\n",
    "        #     torchvision.transforms.Resize((new_height, new_width), interpolation=Image.BICUBIC),  # Image.BICUBIC / InterpolationMode.BICUBIC\n",
    "        #     torchvision.transforms.ToTensor(),\n",
    "        #     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        # ])(x)\n",
    "\n",
    "        preprocessed_image = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((img_size, img_size)),\n",
    "            torchvision.transforms.CenterCrop((img_size, img_size)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            norm\n",
    "        ])(x)\n",
    "\n",
    "        preprocessed_images.append(preprocessed_image)\n",
    "\n",
    "    return torch.stack(preprocessed_images, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the Tasks due to GPU crashs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Featup\n",
    "upsampler = torch.hub.load(\"mhamilton723/FeatUp\", 'dinov2').to('cuda').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"rois_cropped_results.json\"), 'r') as f:\n",
    "    crops_dict = json.load(f)\n",
    "print(\"crops_dict: \", crops_dict.shape) # Shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for d in tqdm(crops_dict):\n",
    "\n",
    "    rois = d[\"rois\"]\n",
    "    sel_rois = d[\"sel_rois\"]\n",
    "    cropped_imgs = d[\"cropped_imgs\"]\n",
    "    cropped_masks = d[\"cropped_masks\"] \n",
    "\n",
    "    scene_features = []\n",
    "\n",
    "    num_imgs = len(cropped_imgs)\n",
    "\n",
    "    \n",
    "    for i in range(num_imgs):\n",
    "        img = cropped_imgs[i]\n",
    "        mask = cropped_masks[i]\n",
    "        # ffa_feature = get_features([img], [mask], encoder, img_size=imsize)\n",
    "        with torch.no_grad:\n",
    "            preprocessed_imgs = FFA_preprocess([img], img_size=imsize).to('cuda')\n",
    "            masks = get_foreground_mask([mask], mask_size=imsize).to('cuda')  # Shape: (1, 1, H, W)\n",
    "            hr_feats = upsampler(preprocessed_imgs)                   # Shape: (1, 384, 512, 512)\n",
    "        hr_feats = torch.nn.functional.interpolate(hr_feats, masks.shape[2:], mode=\"bilinear\") # Shape: (1, 384, H, W)\n",
    "        hr_feats = hr_feats.permute(0,2,3,1) # Shape: (1, H, W, 384)\n",
    "        ffa_feature = (hr_feats * masks.permute(0, 2, 3, 1)).sum(dim=(1, 2)) / masks.sum(dim=(1, 2, 3)).unsqueeze(-1) # Shape: (1, 384)\n",
    "        \n",
    "        # with torch.no_grad():\n",
    "        #     if use_adapter:\n",
    "        #         ffa_feature = adapter(ffa_feature)\n",
    "\n",
    "        scene_features.append(ffa_feature)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    scene_features = torch.cat(scene_features, dim=0)\n",
    "    scene_features = nn.functional.normalize(scene_features, dim=1, p=2)\n",
    "\n",
    "    scene_features_list.append(scene_features) \n",
    "    # total_proposals[scene_name] = sel_rois\n",
    "    proposals_list.append(sel_rois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'object_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m num_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;66;03m# number of instances in the dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m num_example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mobject_features\u001b[49m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_object\n\u001b[0;32m      4\u001b[0m score_thresh_predefined \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.6\u001b[39m\n\u001b[0;32m      6\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'object_features' is not defined"
     ]
    }
   ],
   "source": [
    "num_object = 100 # number of instances in the dataset\n",
    "num_example = len(object_features) // num_object\n",
    "\n",
    "score_thresh_predefined = 0.6\n",
    "\n",
    "results = []\n",
    "    \n",
    "for idx, scene_feature in enumerate(scene_features_list):\n",
    "    sim_mat = compute_similarity(object_features, scene_feature)\n",
    "    sim_mat = sim_mat.view(len(scene_feature), num_object, num_example)\n",
    "    sims, _ = torch.max(sim_mat, dim=2)  # choose max score over profile examples of each object instance\n",
    "\n",
    "    max_ins_sim, initial_result = torch.max(sims, dim=1)\n",
    "\n",
    "    proposals = proposals_list[idx]\n",
    "    num_proposals = len(proposals)\n",
    "\n",
    "    ########################################## Stable Matching Strategy ##########################################\n",
    "\n",
    "    if do_matching:\n",
    "        # ------------ ranking and sorting ------------\n",
    "        # Initialization\n",
    "        sel_obj_ids = [str(v) for v in list(np.arange(num_object))]  # ids for selected obj\n",
    "        sel_roi_ids = [str(v) for v in list(np.arange(len(scene_feature)))]  # ids for selected roi\n",
    "\n",
    "        # Padding\n",
    "        max_len = max(len(sel_roi_ids), len(sel_obj_ids))\n",
    "        sel_sims_symmetric = torch.ones((max_len, max_len)) * -1\n",
    "        sel_sims_symmetric[:len(sel_roi_ids), :len(sel_obj_ids)] = sims.clone()\n",
    "\n",
    "        pad_len = abs(len(sel_roi_ids) - len(sel_obj_ids))\n",
    "        if len(sel_roi_ids) > len(sel_obj_ids):\n",
    "            pad_obj_ids = [str(i) for i in range(num_object, num_object + pad_len)]\n",
    "            sel_obj_ids += pad_obj_ids\n",
    "        elif len(sel_roi_ids) < len(sel_obj_ids):\n",
    "            pad_roi_ids = [str(i) for i in range(len(sel_roi_ids), len(sel_roi_ids) + pad_len)]\n",
    "            sel_roi_ids += pad_roi_ids\n",
    "\n",
    "        # ------------ stable matching ------------\n",
    "        matchedMat = stableMatching(\n",
    "            sel_sims_symmetric.detach().data.cpu().numpy())  # predMat is raw predMat\n",
    "        predMat_row = np.zeros_like(\n",
    "            sel_sims_symmetric.detach().data.cpu().numpy())  # predMat_row is the result after stable matching\n",
    "        Matches = dict()\n",
    "        for i in range(matchedMat.shape[0]):\n",
    "            tmp = matchedMat[i, :]\n",
    "            a = tmp.argmax()\n",
    "            predMat_row[i, a] = tmp[a]\n",
    "            Matches[sel_roi_ids[i]] = sel_obj_ids[int(a)]\n",
    "        # print(\"Done!\")\n",
    "\n",
    "        # ------------ thresholding ------------\n",
    "        preds = Matches.copy()\n",
    "        # for key, value in Matches.items():\n",
    "        #     if sel_sims_symmetric[int(sel_roi_ids.index(key)), int(sel_obj_ids.index(value))] <= score_thresh_predefined:\n",
    "        #         del preds[key]\n",
    "        #         continue\n",
    "        \n",
    "        # ------------ save per scene results ------------\n",
    "\n",
    "        for k, v in preds.items():\n",
    "            if int(k) >= num_proposals:\n",
    "                break\n",
    "            # if float(sims[int(k), int(v)]) < score_thresh_predefined:\n",
    "            #     continue\n",
    "            result = dict()\n",
    "            result['image_id'] = proposals[int(k)]['image_id']\n",
    "            result['category_id'] = int(v)\n",
    "            result['bbox'] = proposals[int(k)]['bbox']\n",
    "            result['score'] = float(sims[int(k), int(v)])\n",
    "            result['image_width'] = proposals[int(k)]['image_width']\n",
    "            result['image_height'] = proposals[int(k)]['image_height']\n",
    "            result['scale'] = proposals[int(k)]['scale']\n",
    "            results.append(result)\n",
    "    else:\n",
    "        THRESHOLD_OBJECT_SCORE = 0.4\n",
    "        for i in range(num_proposals):\n",
    "            if float(max_ins_sim[i]) < THRESHOLD_OBJECT_SCORE:\n",
    "                continue\n",
    "            result = dict()\n",
    "            result['image_id'] = proposals[i]['image_id']\n",
    "            result['category_id'] = initial_result[i].item()\n",
    "            result['bbox'] = proposals[i]['bbox']\n",
    "            result['score'] = float(max_ins_sim[i])\n",
    "            result['image_width'] = proposals[i]['image_width']\n",
    "            result['image_height'] = proposals[i]['image_height']\n",
    "            result['scale'] = proposals[i]['scale']\n",
    "            results.append(result)\n",
    "\n",
    "# Capture the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the total time\n",
    "print(f\"Total running time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ### Save Results\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# save final results\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoco_instances_results.json\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      4\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(results, f)\n\u001b[0;32m      6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m      7\u001b[0m     [(k, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m'\u001b[39m: []}) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(scene_name_list))])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# ### Save Results\n",
    "# save final results\n",
    "with open(os.path.join(args.output_dir, \"coco_instances_results.json\"), \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "predictions = dict(\n",
    "    [(k, {'image_id': -1, 'instances': []}) for k in range(len(scene_name_list))])\n",
    "for idx in range(len(results)):\n",
    "    id = results[idx]['image_id']\n",
    "    predictions[scene_name_list.index('test_' + str(id).zfill(3))]['image_id'] = id\n",
    "\n",
    "    predictions[scene_name_list.index('test_' + str(id).zfill(3))]['instances'].append(results[idx])\n",
    "\n",
    "torch.save(predictions, os.path.join(args.output_dir, \"instances_predictions.pth\"))\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random custom colors with a fixed random seed\n",
    "random.seed(77)\n",
    "thing_colors = []\n",
    "for i in range(100):\n",
    "    thing_colors.append(getColor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Register Test Data for COCO evaluation\n",
    "# test_path = \"./test_data/test_4\" # 1 for raw data, 2 for ratio=0.5, 4 for ratio=0.25, 8 for ratio=0.125  # test_4\n",
    "# test_json = \"./test_data/annotations/instances_test_4.json\"  # instances_test_4\n",
    "test_path = os.path.join(args.test_path, 'images')  # 1 for raw data, 2 for ratio=0.5, 4 for ratio=0.25, 8 for ratio=0.125\n",
    "test_json = os.path.join(args.test_path, 'instances_test_4_'+scene_level+'.json')\n",
    "register_coco_instances(\"coco_InsDet_test\", {}, test_json, test_path)\n",
    "MetadataCatalog.get(\"coco_InsDet_test\").thing_colors = thing_colors\n",
    "\n",
    "## evaluate the results using COCO API\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# Load the ground truth COCO dataset\n",
    "cocoGt = COCO(test_json)\n",
    "\n",
    "# Load your detection results\n",
    "cocoDt = cocoGt.loadRes(os.path.join(args.output_dir, \"coco_instances_results.json\"))\n",
    "\n",
    "# Create a COCOeval object by initializing it with the ground truth and detection results\n",
    "cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "\n",
    "# Run the evaluation\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
